{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is just for creating paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# General Purpose Paths\n",
    "data_small_path = os.path.join('data','data_small.csv')\n",
    "data_big_path = os.path.join('data','data_big.csv')\n",
    "results_path = 'results'\n",
    "\n",
    "# Ex1 - Paths\n",
    "output_rdd_ex1_path = os.path.join(results_path, 'Ex1_rdd.csv')\n",
    "# Ex2 - Paths\n",
    "output_rdd_ex2_path = os.path.join(results_path, 'Ex2_rdd')\n",
    "# Ex3 - Paths\n",
    "output_rdd_ex3_path = os.path.join(results_path, 'Ex3_rdd')\n",
    "\n",
    "# Create result directory from scratch\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Create an inverted index with the following structure:\n",
    "\n",
    "- Continent - Count of occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50 ms, sys: 10 ms, total: 60 ms\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!rm -rf {output_rdd_ex1_path}\n",
    "import pyspark\n",
    "from operator import add\n",
    "\n",
    "def toCSVLine(data):\n",
    "    return ','.join(str(d) for d in data)\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "try :\n",
    "    lines = sc.textFile(data_big_path)\n",
    "    continent_occurences = lines.map( lambda line : \\\n",
    "                                     (line.split(',')[5],int(line.split(',')[9])))\n",
    "    invertedIdx = continent_occurences.reduceByKey(add)\n",
    "    invertedIdx_csv = invertedIdx.map(lambda group: toCSVLine(group))\n",
    "    invertedIdx_csv.saveAsTextFile(output_rdd_ex1_path)\n",
    "    sc.stop()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Create an inverted index with the following structure:\n",
    "- disaster_type : regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70 ms, sys: 20 ms, total: 90 ms\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!rm -rf {output_rdd_ex2_path}\n",
    "\n",
    "import pyspark\n",
    "from operator import add\n",
    "\n",
    "def toCSVLine(data):\n",
    "    return ','.join(str(d) for d in data)\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "try :\n",
    "    lines = sc.textFile(data_big_path)\n",
    "    disasters_regions = lines.map( lambda line : ( line.split(',')[3], line.split(',')[6] ) )\n",
    "    invIndexDisastersRegions = disasters_regions.groupByKey().mapValues(set)\n",
    "    invIndexDisastersRegions_csv = invIndexDisastersRegions.map(lambda group: toCSVLine(group))\n",
    "    invIndexDisastersRegions_csv.saveAsTextFile(output_rdd_ex2_path)\n",
    "    sc.stop()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Using an inverted index, solve: <br>\n",
    "\n",
    "What are the probabilities of getting injured or dying in a natural disaster of type T in the continent C\n",
    "during decade D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80 ms, sys: 30 ms, total: 110 ms\n",
      "Wall time: 34.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pyspark\n",
    "from operator import add\n",
    "\n",
    "def nan_to_zero(a: str)->int:\n",
    "    \"\"\"\n",
    "    Puts a zero string whenever the value is missing\n",
    "    \"\"\"\n",
    "    if not a:\n",
    "        return \"0\"\n",
    "    return a\n",
    "\n",
    "def calculate_proba(num:int,den:int,decimals=3):\n",
    "    if num == den == 0:\n",
    "        return 0\n",
    "    return round(num,den,decimals)\n",
    "\n",
    "def calculate_probas(lists_of_tokens: list):\n",
    "    \"\"\"\n",
    "    calculates probabilities of dying and being injured\n",
    "    regarding total_affected.\n",
    "    \"\"\"\n",
    "    # positions 0 contain each deaths count\n",
    "    total_deaths = sum([tokens[0] for tokens in lists_of_tokens])\n",
    "    # positions 1 contain each injured count\n",
    "    total_injured = sum([tokens[1] for tokens in lists_of_tokens])\n",
    "    # positions 2 contain each total_affected count\n",
    "    total_affected = sum([tokens[2] for tokens in lists_of_tokens])\n",
    "    \n",
    "    if total_injured == total_deaths == total_affected == 0:\n",
    "        death_proba = 0\n",
    "        injured_proba = 0\n",
    "    else:\n",
    "        death_proba = round(total_deaths / total_affected,3)\n",
    "        injured_proba = round(total_injured / total_affected,3)\n",
    "    return [injured_proba, death_proba]\n",
    "\n",
    "def toCSVLine(data):\n",
    "    return ','.join(str(d) for d in data)\n",
    "\n",
    "\n",
    "# list with indices of the relevant columns for this exercise\n",
    "# respectively: 0:decade, 3:disaster, 5:continent, 10:total_deaths, \n",
    "# 11:injured, 12:affected, 13:homeless, 14:total_affected.\n",
    "columns = [0,3,5,10,11,12,13,14]\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "try :\n",
    "    lines = sc.textFile(data_big_path)\n",
    "    tokenized = lines.map(lambda line: line.split(','))\n",
    "    # select relevant columns for this exercise\n",
    "    relevant_tokens = tokenized.map(lambda tokens: [token for idx, token in enumerate(tokens) if idx in columns])\n",
    "    # convert missing values to \"0\"\n",
    "    fix_nans = relevant_tokens.map(lambda tokens: [nan_to_zero(e) for e in tokens])\n",
    "    # convert numeric fields from string to int\n",
    "    fix_ints = fix_nans.map(lambda tokens: tokens[:3] + [int(e) for e in tokens[3:]])\n",
    "    # convert years to decades\n",
    "    fix_decades = fix_ints.map(lambda tokens: [tokens[0][:-1]+'0'] + tokens[1:])\n",
    "    \n",
    "    # add a new field that is the sum of total_deaths, \n",
    "    # injured, affected and homeless for each row.\n",
    "    custom_total_affected = fix_decades.map(lambda tokens:\\\n",
    "                                         tokens + [sum([token for token in tokens[3:7]])])\n",
    "    \n",
    "    # if the total_affected field is smaller than the sum of each \n",
    "    # type of affected, then replace it's value with the sum.\n",
    "    fixed_total_affected = custom_total_affected.map(lambda tokens: \\\n",
    "                            tokens[:-1] if tokens[-2] >= tokens[-1] else tokens[:-2] + [tokens[-1]])\n",
    "    \n",
    "    # dataset has been preprocessed, we can start solving the exercise\n",
    "    \n",
    "    # create a single key by concatenating decade with continent with disaster\n",
    "    # and vales are a list : [death_count, injured_count, total_affected_count]\n",
    "    key_value = fixed_total_affected.map(lambda tokens: (f\"{tokens[0]}_{tokens[1]}_{tokens[2]}\", [tokens[3],tokens[4],tokens[-1]]))\n",
    "    groups = key_value.groupByKey()\n",
    "    probas = groups.mapValues(calculate_probas)\n",
    "    sorted_keys = probas.sortByKey(ascending=True)\n",
    "    \n",
    "    sorted_keys_csv = sorted_keys.map(lambda group: toCSVLine(group))\n",
    "    sorted_keys_csv.saveAsTextFile(output_rdd_ex3_path)\n",
    "    \n",
    "    sc.stop()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
